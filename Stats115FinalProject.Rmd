---
title: "Stats 115 Final Project"
authors: "Jerry Xu, Saadia Karim, Brady Hong"
output: html_document
---

```{r}
library(bayesrules)
library(tidyverse)
library(janitor)
library(rstan)
library(bayesplot)
library(rstanarm)
```

Are popular songs explicit?

Articles are saying that explicit music is dominating the music industry, so let's see what we can gather to predict a posterior beta distribution of the proportion of songs that are explicit in popular music, given data from spotify's music data, and a prior based on research articles.

Step 1: 
Getting data on popular music in Spotify.

```{r}
spot_data <- read.csv(file = 'data.csv')
```

```{r}
head(spot_data)
```

We want to look at the popularity of a song first, to retrieve the most popular songs. 

```{r}
popularSongsData <- spot_data %>%
  filter(popularity > 85)
head(popularSongsData)
```
```{r}
popularSongsData
```

Step 2 - approximating the prior:

Research suggests that the proportion of explicit content in popular music lies between 0.68 - 0.84, so let's find a prior beta model that matches it, with an expectation of 0.76.

a / a + B = 0.76

a = 0.76a + 0.76B
0.24a = 0.76B
a = 3.17B

Let's try a few Beta models that have Alpha as 3.17*Beta, and it seems to be between 0.68 and 0.84.


```{r}
plot_beta(9.5, 3)
summarize_beta(9.5, 3)
```

```{r}
plot_beta(222, 70)
summarize_beta(222, 70)
```

This model seems to be at the right place, ranging from 0.68-0.84, centered around 0.76. This implies that around 76% of popular music tends to be explicit.

95% Posterior Credible Interval (CI):

```{r}
qbeta(c(0.025, 0.975), 222, 70)
```
Seems as though our prior was higher than the posterior, and our posterior now says that there is a 95% chance that the proportion of explicit songs in popular music lies between 60.5% and 69.8%. 

Step 3-
Calculating the posterior:
```{r}
popularSongsData %>% 
  tabyl(explicit)
```

Seems that 50 out of 124 popular songs are explicit in our dataset. 

```{r}
plot_beta_binomial(222, 70, y = 50, n = 124)
```

```{r}
summarize_beta_binomial(222, 70, y = 50, n = 124)
```

95% Posterior Credible Interval (CI):

```{r}
qbeta(c(0.025, 0.975), 272, 144)
```

Seems as though our prior was higher than the posterior, and our posterior now says that there is a 95% chance that the proportion of explicit songs in popular music lies between 60.5% and 69.8%. 


Step 4-
Simulating through Rstan

```{r}
bb_model <- "
data {
int < lower=0, upper=100> Y;
}
parameters{
real<lower = 0, upper=1> pi;
}
model{
Y~binomial(124, pi);
pi~beta(222, 70);
}"
```

```{r}
explicitSongs_sim <- stan(
  model_code = bb_model, data= list(Y=50),
  chains = 4, iter = 5000*2, seed = 84735)
```

```{r}
mcmc_dens(explicitSongs_sim, pars = "pi")
```

```{r}
summary(explicitSongs_sim, pars = "pi")$summary
```

The 95% Posterior Credible Interval (CI) is almost the same as the approxmation one done before, and it is clear that the posterior now says that there is a 95% chance that the proportion of explicit songs in popular music lies between 60.7% and 69.8%.



Hypothesis Testing



Our goal is to run a hypothesis test on whether or not the most popular songs have a high danceability. Since the most popular songs are catchy and easy to memorize, we believe that they will also tend to have a higher danceability value.

```{r}
#We want to analyze the 100 most popular songs for our bayesian analysis, so we order by most popular and take the top 100 rows.
most_popular <- arrange(spot_data, -popularity)
most_pop_100<- head(most_popular, 100)

#Based on https://towardsdatascience.com/what-makes-a-song-likeable-dbfdb7abe404, We predict that approximately 2/3 of our observational data will be considered high danceability.
plot_beta(alpha = 8, beta = 4)
summarize_beta(alpha=8,beta=4)

#Based on https://towardsdatascience.com/what-makes-a-song-likeable-dbfdb7abe404, we consider a danceability of 0.6 to be high and will classify them into either high or low danceability so we can simulate the hypothesis test with a beta-binomial model.
most_pop_100 %>% 
  mutate(highDance = (danceability  >= 0.60)) %>% 
  tabyl(highDance)
```
We see that in our data, 76 of our observations did indeed have a danceability of over 0.6 which is higher than our prior prediction so now we will visualize it using a beta-binomial plot to see it's effect on the posterior.

```{r}
#
plot_beta_binomial(alpha = 8, beta = 4, y = 76, n = 100)
summarize_beta_binomial(alpha = 8, beta = 4, y = 76, n = 100)
```
Based on the graph and summary, it looks like our posterior is closer to the data. The posterior pulled the likelihood closer to itself than the prior, which also increased the mean and mode.

```{r}
#95% posterior credible interval check
qbeta(c(0.025, 0.975), 84, 28)
#This function is the same as if we were to integrate from 0.6661702  to 0.8253767 of f(pi|(y=76))dpi, and ~0.95 or 95% of the area would be under the curve.
#There is a 95% chance that the true mean proportion of songs with a danceability of 0.6 or higher is between 0.6661702 and 0.8253767, which is in our beta binomial distribution.
```


Running the actual hypothesis test. The null hypothesis (Ho) is we predict that less than 2/3 of the most popular 100 songs will meet our criteria that their danceability is less than 0.6. The alternative hypothesis is that at least 2/3 of the most popular 100 songs will have a danceability of >0.6
Ho: pi < 0.66
Ha: pi >= 0.66


```{r}
#Calculating the posterior probability and posterior odds
post_prob <- pbeta(0.66,  84, 28, lower.tail = FALSE)
post_prob

post_odds <- post_prob/(1-post_prob)#prob alternative over prob null using posterior model
post_odds

#Calculating prior probability and prior odds
prior_prob <- pbeta(0.66, 8, 4, lower.tail = FALSE)
prior_prob

prior_odds <- prior_prob/(1-prior_prob) #probability of the alternative hypothesis to the null hypothesis
prior_odds

bayesFactor <- post_odds/prior_odds # Bayes factor = (posterior odds)/(prior odds)
bayesFactor
```
Since Bayes Factor is > 1, after observing the data the plausibility of Ha increased as a result of the observed data. We can reject the null because we do have convincing evidence that more than 2/3s of the most popular songs have a danceability greater than 0.6



Linear regression



## Creating random sample dataset

Storing 100 random sample of data into spot_data

```{r}
set.seed(84735)
spot_data <- as.data.frame(spot_data) %>%
  sample_n(size = 100)
```

## Building model 0

Building model 0 with features acousticness, danceability, duration_ms, energy, loudness, and tempo.

```{r}
spot_model_0 <- stan_glm(popularity ~ acousticness + danceability + duration_ms + energy + loudness + tempo, data = spot_data,
family = gaussian, chains = 4,
iter = 5000*2, seed = 84735, refresh=FALSE)
```

## Checking MAE and mae_scaled by k-fold cross validation algorithm

```{r}
prediction_summary_cv(data = spot_data, model = spot_model_0, k = 10)
```

## 80 percent posterior credible interval for model 0

```{r}
posterior_interval(spot_model_0, prob = 0.80)
```

## Building model 1

Building model1 with features danceability, energy, loudness, and tempo.

```{r}
spot_model_1 <- stan_glm(popularity ~ danceability + energy + loudness + tempo, data = spot_data,
family = gaussian, chains = 4,
iter = 5000*2, seed = 84735, refresh=FALSE)
```

## Checking MAE and mae_scaled by k-fold cross validation algorithm for model 1

MAE and mae_scaled actually went up compared to model 0. So it is hard to say it is better model than model 1

```{r}
prediction_summary_cv(data = spot_data, model = spot_model_1, k = 10)
```

## 80 percent posterior credible interval for model 1

```{r}
posterior_interval(spot_model_1, prob = 0.80)
```

## Building model 2

Getting rid of loudness and tempo feature for model 2 because those features don't seem like have much association.

```{r}
spot_model_2 <- stan_glm(popularity ~ danceability + energy, data = spot_data,
family = gaussian, chains = 4,
iter = 5000*2, seed = 84735, refresh=FALSE)
```

## Checking MAE and mae_scaled by k-fold cross validation algorithm for model 2

Doesn't show much better mae and mae_scaled than past 2 models.

```{r}
prediction_summary_cv(data = spot_data, model = spot_model_2, k = 10)
```

## 80 percent posterior credible interval for model 2

```{r}
posterior_interval(spot_model_2, prob = 0.80)
```

## Building model 3

```{r}
spot_model_3 <- stan_glm(popularity ~ acousticness + danceability + energy + valence +liveness +speechiness, data = spot_data,
family = gaussian, chains = 4,
iter = 5000*2, seed = 84735, refresh=FALSE)
```

## Checking MAE and mae_scaled by k-fold cross validation algorithm for model 3

Lowest mae and mae_scaled so far

```{r}
prediction_summary_cv(data = spot_data, model = spot_model_3, k = 10)
```

## 80 percent posterior credible interval for model 3

```{r}
posterior_interval(spot_model_3, prob = 0.80)
```

## Building model 4

Removed liveness, and energy because they don't show strong association compared to other features.

```{r}
spot_model_4 <- stan_glm(popularity ~ acousticness + danceability + valence + speechiness, data = spot_data,
family = gaussian, chains = 4,
iter = 5000*2, seed = 84735, refresh=FALSE)
```

## Checking MAE and mae_scaled by k-fold cross validation algorithm for model 4

We got the lowest mae and mae_scaled out of 5 models.

```{r}
prediction_summary_cv(data = spot_data, model = spot_model_4, k = 10)
```


## 80 percent posterior credible interval for model 4

Acousticness, valence, and speechiness has negative association and danceability have positive association.

```{r}
posterior_interval(spot_model_4, prob = 0.80)
```

## Posterior Predictive Check for the model

```{r}
pp_check(spot_model_4)
```

We can conclude from the plot that the prediction is good enough. 

## Trace plot of model 4

```{r}
mcmc_trace(spot_model_4, size = 0.1)
```

acousticness mostly between -8 to -20
danceability mostly between 25 to 75
valence mostly between -5 to -25
speechiness mostly between -10 to -45