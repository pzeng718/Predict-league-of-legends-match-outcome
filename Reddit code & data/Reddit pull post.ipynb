{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "reddit_client_id = \"r4RQ29Ib_bGl4A\"\n",
    "reddit_secret_key = \"fMtsTTdf8NG4lIJPbl7X-C_2biB-0g\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id = reddit_client_id,\n",
    "    client_secret = reddit_secret_key,\n",
    "    user_agent = \"PeterPan12138\"\n",
    ")\n",
    "reddit.read_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read existing post into table\n",
    "posts = pd.read_csv(r'../data/posts.csv')\n",
    "comments = pd.read_csv(r'../data/comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"leagueoflegends\").hot(limit = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data gathering\n",
    "start = time.time()\n",
    "user_ids = []\n",
    "user_names = []\n",
    "time_stamps = []\n",
    "texts = []\n",
    "post_ids = []\n",
    "comments = []\n",
    "num_upvotes = []\n",
    "\n",
    "for submission in subreddit:\n",
    "    if submission.selftext != \"\":\n",
    "        try:\n",
    "            user_ids.append(submission.author.id)\n",
    "            user_names.append(submission.author.name)\n",
    "            time_stamps.append(datetime.fromtimestamp(submission.created_utc))\n",
    "            texts.append(submission.selftext)\n",
    "            post_ids.append(submission.id)\n",
    "            comments.append(submission.comments)\n",
    "            num_upvotes.append(submission.score)\n",
    "        except:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_posts = pd.DataFrame({\n",
    "            'user_id' : user_ids,\n",
    "            'user_name' : user_names,\n",
    "            'text' : texts,\n",
    "            'time' : time_stamps,\n",
    "            'num_upvotes' : num_upvotes,\n",
    "            'post_id' : post_ids\n",
    "        })\n",
    "end = time.time()\n",
    "print('It took {} seconds to get {} posts.'.format(end - start, len(post_ids)))\n",
    "posts = posts.append(new_posts, ignore_index = True)\n",
    "posts.drop_duplicates(subset=\"post_id\", keep=\"first\", inplace=True)\n",
    "print('Now there are {} posts'.format(len(posts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.to_csv(r'../data/posts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_posts = pd.read_csv(r'../data/comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comment_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the comments\n",
    "\n",
    "comment_post_ids = []\n",
    "comment_ids = []\n",
    "comment_texts = []\n",
    "comment_time_stamps = []\n",
    "for post_id_index, post_id in enumerate(post_ids):\n",
    "    for comment in comments[post_id_index]:\n",
    "        if type(comment) is praw.models.Comment:\n",
    "            comment_post_ids.append(post_id)\n",
    "            comment_ids.append(comment.id)\n",
    "            comment_texts.append(comment.body)\n",
    "            comment_time_stamps.append(datetime.fromtimestamp(comment.created_utc))\n",
    "# Only getting the comments in the first layer\n",
    "\n",
    "#         elif type(comment) is praw.models.MoreComments:\n",
    "#             for more_comment in comment.comments():\n",
    "#                 print(post_id_index)\n",
    "#                 comment_post_ids.append(post_id)\n",
    "#                 comment_ids.append(more_comment.id)\n",
    "#                 comment_texts.append(more_comment.body)\n",
    "#                 comment_time_stamps.append(datetime.fromtimestamp(more_comment.created_utc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_comment_posts = pd.DataFrame({\n",
    "    'post_id' : comment_post_ids,\n",
    "    'comment_id' : comment_ids,\n",
    "    'comment_text' : comment_texts,\n",
    "    'time' : comment_time_stamps\n",
    "})\n",
    "comment_posts = comment_posts.append(new_comment_posts, ignore_index = True)\n",
    "comment_posts.drop_duplicates(subset=\"comment_id\", keep=\"first\", inplace=True)\n",
    "comment_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_posts.to_csv(r'../data/comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data wrangling\n",
    "import nltk\n",
    "import re\n",
    "from gensim.models import word2vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk import WordNetLemmatizer  \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "nltk.downloader.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec\n",
    "\n",
    "post_corpus = [post_text for post_text in posts.text]\n",
    "comment_corpus = [comment_text for comment_text in comment_posts.comment_text]\n",
    "\n",
    "post_tok_corp = [nltk.word_tokenize(sentence) for sentence in post_corpus]\n",
    "comment_tok_corp = [nltk.word_tokenize(sentence) for sentence in comment_corpus if isinstance(sentence, str)]\n",
    "\n",
    "def lemmatize_word(word):\n",
    "    return wordnet_lemmatizer.lemmatize(word)\n",
    "\n",
    "# To remove words that we do not need to look at\n",
    "def remove_stopwords(target_list):\n",
    "    self_defined_stopwords = ['champion', 'play', 'item', 'even', 'one', 'team', 'really', \n",
    "                              'player', 'time', 'know', 'people', 'make', 'game', 'think', \n",
    "                              'wa', 'ha', 'champ', 'new', 'doe', 'feel', 'want',\n",
    "                              'enemy', 'see', 'playing', 'way', 'need', 'still', 'riot', \n",
    "                              'support', 'much', 'lot', 'league', 'every', 'first',\n",
    "                              'jungle', 'go', 'thing', 'give', 'back', 'gwen', 'something',\n",
    "                              'season', 'top', 'got', 'build', 'say', 'season','played',\n",
    "                              'take', 'skin', 'lane', 'ap', 'change', 'mid', 'many', 'adc',\n",
    "                              'second', 'level', 'ability', 'someone', 'right', 'match']\n",
    "    result = re.compile('^[a-zA-Z]{2,}$')\n",
    "    target_list = [lemmatize_word(word.lower()) for word in target_list]\n",
    "    return [word for word in target_list if result.match(word) and word not in stopwords.words('english') and word not in self_defined_stopwords]\n",
    "\n",
    "post_tok_corp = [remove_stopwords(word_list) for word_list in post_tok_corp]\n",
    "comment_tok_corp = [remove_stopwords(word_list) for word_list in comment_tok_corp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_model = word2vec.Word2Vec(post_tok_corp, min_count=10, sg = 1)\n",
    "comment_model = word2vec.Word2Vec(comment_tok_corp, min_count=10, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champions = pd.read_csv(r'../data/champions.txt', delimiter = \",\", header=None).values.tolist()[0]\n",
    "champions = [champion.lower() for champion in champions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_names_post = []\n",
    "champion_occur_times = []\n",
    "for sentence in post_tok_corp:\n",
    "    for word in sentence:\n",
    "        if word in champions:\n",
    "            champion_names_post.append(word)\n",
    "\n",
    "champion_names_comment = []\n",
    "for sentence in comment_tok_corp:\n",
    "    for word in sentence:\n",
    "        if word in champions:\n",
    "            champion_names_comment.append(word)\n",
    "            \n",
    "champ_str_post = \" \".join(champion_names_post)\n",
    "champ_str_comment = \" \".join(champion_names_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the word cloud\n",
    "# Data visualization\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate(champ_str_post)\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.title(\"Popular champions based on post\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate(champ_str_comment)\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.title(\"Popular champions based on comments\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_tok_sentence = ''\n",
    "for sentence in post_tok_corp:\n",
    "    post_tok_sentence += ' '.join(sentence)\n",
    "    \n",
    "comment_tok_sentence = ''\n",
    "for sentence in comment_tok_corp:\n",
    "    comment_tok_sentence += ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate(post_tok_sentence)\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Popular words after manually removing stopwords\")\n",
    "plt.tight_layout(pad = 0)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate(comment_tok_sentence)\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_champions = ['orianna', 'gnar', 'thresh', 'alistar', 'tristana', 'hecarim', 'udyr']\n",
    "def create_word_cloud(champion):\n",
    "    champion_sentence = champion_related_sentence[champions.index(champion)]\n",
    "    wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                min_font_size = 10).generate(champion_sentence)\n",
    "    plt.figure(figsize = (8, 8), facecolor = None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.xlabel('Wordcloud for {}'.format(champion))\n",
    "    plt.tight_layout(pad = 0)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Showing wordcloud for {}\".format(champion))\n",
    "    \n",
    "for champion in popular_champions:\n",
    "    create_word_cloud(champion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building and training\n",
    "def get_occur_times(corpus):\n",
    "    occur_dict = dict()\n",
    "    for word_list in corpus:\n",
    "        for word in word_list:\n",
    "            for champion in champions:\n",
    "                if word.lower() == champion.lower():\n",
    "                    if champion in occur_dict:\n",
    "                        occur_dict[champion] += 1\n",
    "                    else:\n",
    "                        occur_dict[champion] = 1\n",
    "                         \n",
    "    occur_list = [0 for i in range(len(champions))]\n",
    "    for champion in champions:\n",
    "        if champion in occur_dict.keys():\n",
    "            occur_list[champions.index(champion)] = occur_dict[champion]\n",
    "        else:\n",
    "            occur_list[champions.index(champion)] = 0\n",
    "    return occur_list\n",
    "post_occur_list = get_occur_times(post_tok_corp)\n",
    "comment_occur_list = get_occur_times(comment_tok_corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_champions(keyword, model):\n",
    "    result = []\n",
    "    for score_pair in model.wv.most_similar(keyword, topn=1500):\n",
    "        if score_pair[0] in champions:\n",
    "            result.append(score_pair)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_keyword = ['powerful', 'broken', 'strong', 'good', 'better', 'well', 'damage', 'snowball', 'best']\n",
    "negative_keyword = ['useless', 'weak', 'bad']\n",
    "post_positive_realted_champions = []\n",
    "post_negative_realted_champions = []\n",
    "comment_positive_realted_champions = []\n",
    "comment_negative_realted_champions = []\n",
    "\n",
    "def populate_champions(keywords, champions_list, model):\n",
    "    for keyword in keywords:\n",
    "        champions_list.append(get_related_champions(keyword, model))\n",
    "\n",
    "populate_champions(positive_keyword, post_positive_realted_champions, post_model)\n",
    "populate_champions(negative_keyword, post_negative_realted_champions, post_model)\n",
    "populate_champions(positive_keyword, comment_positive_realted_champions, comment_model)\n",
    "populate_champions(negative_keyword, comment_negative_realted_champions, comment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "post_positive_counts = np.array([len(word_list) for word_list in post_positive_realted_champions])\n",
    "post_negative_counts = np.array([len(word_list) for word_list in post_negative_realted_champions])\n",
    "post_positive_counts_norm = np.ndarray.tolist(preprocessing.normalize([post_positive_counts]))[0]\n",
    "post_negative_counts_norm = np.ndarray.tolist(preprocessing.normalize([post_negative_counts]))[0]\n",
    "\n",
    "comment_positive_counts = np.array([len(word_list) for word_list in comment_positive_realted_champions])\n",
    "comment_negative_counts = np.array([len(word_list) for word_list in comment_negative_realted_champions])\n",
    "comment_positive_counts_norm = np.ndarray.tolist(preprocessing.normalize([comment_positive_counts]))[0]\n",
    "comment_negative_counts_norm = np.ndarray.tolist(preprocessing.normalize([comment_negative_counts]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_scores = dict()\n",
    "comment_scores = dict()\n",
    "\n",
    "def populate_scores(scores, positive_realted_champions, negative_realted_champions, positive_counts_norm, negative_counts_norm):\n",
    "    for i in range(len(positive_keyword)):\n",
    "        champion_scores = positive_realted_champions[i]\n",
    "        for score_pair in champion_scores:\n",
    "            champion_name = score_pair[0]\n",
    "            score = score_pair[1]\n",
    "            if champion_name in scores:\n",
    "                scores[champion_name] = scores[champion_name] + positive_counts_norm[i]*score\n",
    "            else:\n",
    "                scores[champion_name] = positive_counts_norm[i]*score\n",
    "\n",
    "    for i in range(len(negative_keyword)):\n",
    "        champion_scores = negative_realted_champions[i]\n",
    "        for score_pair in champion_scores:\n",
    "            champion_name = score_pair[0]\n",
    "            score = score_pair[1]\n",
    "            if champion_name in scores:\n",
    "                if isinstance(scores[champion_name], float):\n",
    "                    scores[champion_name] = (scores[champion_name], negative_counts_norm[i]*score)\n",
    "                else:\n",
    "                    scores[champion_name] = (scores[champion_name][0], scores[champion_name][1] + negative_counts_norm[i]*score)\n",
    "            else:\n",
    "                scores[champion_name] = (0, negative_counts_norm[i]*score)\n",
    "\n",
    "    for key in scores.keys():\n",
    "        if isinstance(scores[key], float):\n",
    "            scores[key] = (scores[key] / sum(positive_counts_norm), 0)\n",
    "        else:\n",
    "            scores[key] = (scores[key][0] / sum(positive_counts_norm), scores[key][1] / sum(negative_counts_norm))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "post_scores = populate_scores(post_scores, post_positive_realted_champions, post_negative_realted_champions, post_positive_counts_norm, post_negative_counts_norm)\n",
    "comment_scores = populate_scores(comment_scores, comment_positive_realted_champions, comment_negative_realted_champions, comment_positive_counts_norm, comment_negative_counts_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_positive_scores = [0 for i in range(len(champions))]\n",
    "post_negative_scores = [0 for i in range(len(champions))]\n",
    "comment_positive_scores = [0 for i in range(len(champions))]\n",
    "comment_negative_scores = [0 for i in range(len(champions))]\n",
    "\n",
    "def generate_score_list(scores, positive_scores, negative_scores):\n",
    "    for champion_name in scores.keys():\n",
    "        pos_score = scores[champion_name][0]\n",
    "        neg_score = scores[champion_name][1]\n",
    "\n",
    "        positive_scores[champions.index(champion_name)] = pos_score\n",
    "        negative_scores[champions.index(champion_name)] = neg_score\n",
    "\n",
    "generate_score_list(post_scores, post_positive_scores, post_negative_scores)\n",
    "generate_score_list(comment_scores, comment_positive_scores, comment_negative_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the results to be put in the final table\n",
    "champion_id_name_dict = {266: 'Aatrox', 103: 'Ahri', 84: 'Akali', 12: 'Alistar', 32: 'Amumu',34: 'Anivia',1: 'Annie',523: 'Aphelios', 22: 'Ashe', \\\n",
    "               136: 'Aurelion Sol',268: 'Azir' ,432: 'Bard',53: 'Blitzcrank', 63: 'Brand', 201: 'Braum',51: 'Caitlyn',164: 'Camille' ,\\\n",
    "               69: 'Cassiopeia',31: \"Cho'Gath\",42: 'Corki',122: 'Darius',131: 'Diana',119: 'Draven',36: 'Dr. Mundo',245: 'Ekko', \\\n",
    "               60: 'Elise',28: 'Evelynn',81: 'Ezreal',9: 'Fiddlesticks',114: 'Fiora',105: 'Fizz',3: 'Galio',41: 'Gangplank', \\\n",
    "               86: 'Garen',150: 'Gnar', 79: 'Gragas',104: 'Graves',120: 'Hecarim',74: 'Heimerdinger',420: 'Illaoi',39: 'Irelia',\\\n",
    "               427: 'Ivern',40: 'Janna',59: 'Jarvan IV', 24: 'Jax', 126: 'Jayce', 202: 'Jhin', 222: 'Jinx',145: \"Kai'Sa\", \\\n",
    "               429: 'Kalista', 43: 'Karma', 30: 'Karthus', 38: 'Kassadin', 55: 'Katarina', 10: 'Kayle', 141: 'Kayn', 85: 'Kennen', \\\n",
    "               121: \"Kha'Zix\",203: 'Kindred',240: 'Kled',96: \"Kog'Maw\",7: 'LeBlanc',64: 'Lee Sin',89: 'Leona',876: 'Lillia', \\\n",
    "               127: 'Lissandra', 236: 'Lucian',117: 'Lulu',99: 'Lux',54: 'Malphite',90: 'Malzahar',57: 'Maokai',11: 'Master Yi', \\\n",
    "               21: 'Miss Fortune',62: 'Wukong',82: 'Mordekaiser',25: 'Morgana',267: 'Nami',75: 'Nasus',111: 'Nautilus',518: 'Neeko', \\\n",
    "               76: 'Nidalee',56: 'Nocturne',20: 'Nunu and Willump',2: 'Olaf',61: 'Orianna',516: 'Ornn',80: 'Pantheon',78: 'Poppy', \\\n",
    "               555: 'Pyke',246: 'Qiyana',133: 'Quinn',497: 'Rakan',33: 'Rammus',421: \"Rek'Sai\",526: 'Rell',58: 'Renekton', \\\n",
    "               107: 'Rengar',92: 'Riven',68: 'Rumble',13: 'Ryze',360: 'Samira',113: 'Sejuani',235: 'Senna',147: 'Seraphine', \\\n",
    "               875: 'Sett',35: 'Shaco',98: 'Shen',102: 'Shyvana',27: 'Singed',14: 'Sion',15: 'Sivir',72: 'Skarner',37: 'Sona', \\\n",
    "               16: 'Soraka',50: 'Swain', 517: 'Sylas',134: 'Syndra',223: 'Tahm Kench',163: 'Taliyah',91: 'Talon',44: 'Taric', \\\n",
    "               17: 'Teemo',412: 'Thresh',18: 'Tristana',48: 'Trundle',23: 'Tryndamere',4: 'Twisted Fate',29: 'Twitch',77: 'Udyr', \\\n",
    "               6: 'Urgot',110: 'Varus',67: 'Vayne',45: 'Veigar',161: \"Vel'Koz\",254: 'Vi',234: 'Viego',112: 'Viktor',8: 'Vladimir',\\\n",
    "               106: 'Volibear',19: 'Warwick',498: 'Xayah',101: 'Xerath',5: 'Xin Zhao',157: 'Yasuo',777: 'Yone',83: 'Yorick', \\\n",
    "               350: 'Yuumi',154: 'Zac',238: 'Zed',115: 'Ziggs',26: 'Zilean',142: 'Zoe',143: 'Zyra', 887: 'Gwen'}\n",
    "champion_ids = [0 for i in range(len(champions))]\n",
    "for id in champion_id_name_dict.keys():\n",
    "    champion_name = champion_id_name_dict[id].lower()\n",
    "    champion_ids[champions.index(champion_name)] = id\n",
    "champion_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_strength_info = pd.DataFrame({\n",
    "    'champion_name': champions,\n",
    "    'champion_id': champion_ids,\n",
    "    'post_mentioned_times': post_occur_list,\n",
    "    'comment_mentioned_times': comment_occur_list,\n",
    "    'post_positive_score': post_positive_scores,\n",
    "    'post_negative_score': post_negative_scores,\n",
    "    'comment_positive_score': comment_positive_scores,\n",
    "    'comment_negative_score': comment_negative_scores\n",
    "})\n",
    "champion_strength_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_strength_info.to_csv(r'/Users/zengpeifan/Stats 170A/Final Project/Data/champion_strength.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random sample of 100 observations for validation\n",
    "sample_matches_wr = matches_wr.sample(n = 100)\n",
    "sample_matches_wr.to_csv(r'/Users/zengpeifan/Stats 170A/Final Project/Data/sample_final_match_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateCMTable(target_type):\n",
    "    for team in [\"Team1\", \"Team2\"]:\n",
    "        for player in [\"P1\", \"P2\", \"P3\", \"P4\", \"P5\"]:\n",
    "            occur_times = []\n",
    "            positive_scores = []\n",
    "            negative_scores = []\n",
    "\n",
    "            for c_id in matches_wr[team + player]:\n",
    "                occur_times.append(champion_strength_info[champion_strength_info['champion_id'] == c_id][target_type.lower() + '_mentioned_times'].values[0])\n",
    "                positive_scores.append(champion_strength_info[champion_strength_info['champion_id'] == c_id][target_type.lower() + '_positive_score'].values[0])\n",
    "                negative_scores.append(champion_strength_info[champion_strength_info['champion_id'] == c_id][target_type.lower() + '_negative_score'].values[0])\n",
    "\n",
    "            matches_wr[team + player + target_type + 'ChampOccurTimes'] = occur_times\n",
    "            matches_wr[team + player + target_type + 'PositiveScore'] = positive_scores\n",
    "            matches_wr[team + player + target_type + 'NegativeScore'] = negative_scores\n",
    "            \n",
    "\n",
    "updateCMTable('Post')\n",
    "updateCMTable('Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_wr.to_csv(r'/Users/zengpeifan/Stats 170A/Final Project/Data/CMWRLP1_ChampStrength.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample = sample_matches_wr['TeamTwoWin']\n",
    "X_sample = sample_matches_wr[['T1P1LP', 'T1P2LP', 'T1P3LP', 'T1P4LP', 'T1P5LP', 'T2P1LP', 'T2P2LP', 'T2P3LP', 'T2P4LP', 'T2P5LP',\n",
    "                'Team1P1PostChampOccurTimes', 'Team1P2PostChampOccurTimes', 'Team1P3PostChampOccurTimes', 'Team1P4PostChampOccurTimes', 'Team1P5PostChampOccurTimes', \n",
    "                'Team2P1PostChampOccurTimes', 'Team2P2PostChampOccurTimes', 'Team2P3PostChampOccurTimes', 'Team2P4PostChampOccurTimes', 'Team2P5PostChampOccurTimes',\n",
    "                'Team1P1PostPositiveScore', 'Team1P2PostPositiveScore', 'Team1P3PostPositiveScore', 'Team1P4PostPositiveScore', 'Team1P5PostPositiveScore', \n",
    "                'Team2P1PostPositiveScore', 'Team2P2PostPositiveScore', 'Team2P3PostPositiveScore', 'Team2P4PostPositiveScore', 'Team2P5PostPositiveScore',\n",
    "                'Team1P1CommentNegativeScore', 'Team1P2CommentNegativeScore', 'Team1P3CommentNegativeScore', 'Team1P4CommentNegativeScore', 'Team1P5CommentNegativeScore', \n",
    "                'Team2P1CommentNegativeScore', 'Team2P2CommentNegativeScore', 'Team2P3CommentNegativeScore', 'Team2P4CommentNegativeScore', 'Team2P5CommentNegativeScore',\n",
    "                'T1P1ChampWR', 'T1P2ChampWR', 'T1P3ChampWR', 'T1P4ChampWR', 'T1P5ChampWR',\n",
    "                'T2P1ChampWR', 'T2P2ChampWR', 'T2P3ChampWR', 'T2P4ChampWR', 'T2P5ChampWR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample_pred = clf.predict(X_sample)\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_sample, y_sample_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
